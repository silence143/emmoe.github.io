<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation in Open Environments">
  <meta name="keywords" content="Embodied AI, Benchmark, Mobile Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation in Open Environments</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./css/bulma.min.css">
  <link rel="stylesheet" href="./css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./css/bulma-slider.min.css">
  <link rel="stylesheet" href="./css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./css/index.css">
  <!-- <link rel="icon" href="./images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./js/fontawesome.all.min.js"></script>
  <script src="./js/bulma-carousel.min.js"></script>
  <script src="./js/bulma-slider.min.js"></script>
  <script src="./js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
  </nav>

  <!-- headline -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation
              in Open Environments</h1>
            <p style="color: darkred; font-weight: bold;">Autonomous household robots driven by user instructions.</p>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://silence143.github.io" target="_blank">Dongping Li<sup>1,2</sup><sup>*†</sup>
                </a>,
              </span>
              <span class="author-block">
                Tielong Cai<sup>1</sup><sup>*</sup>,
              </span>
              <span class="author-block">
                Tianci Tang<sup>1</sup><sup>*</sup>,
              </span>
              <span class="author-block">
                <a href="https://rese1f.github.io/" target="_blank">Wenhao Chai<sup>3</sup></a>,
              </span>
              <br>
              <span class="author-block">
                <a href="https://krdc.web.illinois.edu/" target="_blank">Katherine Rose Driggs-Campbell<sup>2</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://cvnext.github.io/" target="_blank">Gaoang Wang<sup>1</sup></a>
              </span>
            </div>


            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Zhejiang University</span>
              <span class="author-block"><sup>2</sup>University of Illinois Urbana-Champaign</span>
              <span class="author-block"><sup>3</sup>University of Washington</span>
              <br>
              <p class="is-size-6"><sup>*</sup>Equal Contribution <sup>†</sup>Project Lead</p>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="assets/EMMOE.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2503.08604" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/silence143/EMMOE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Model Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/collections/Dongping-Li/emmoe-dataset-and-model-67c6b04da2b83b08ec273ef2"
                    target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face"
                        style="width: 20px; height: 18px; margin-right: 5px;">
                    </span>
                    <span>Model</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/Dongping-Li/EMMOE-100" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face"
                        style="width: 20px; height: 18px; margin-right: 5px;">
                    </span>
                    <span>Data</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Demo and Abstract -->
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Full Video</h2>
          <div class="publication-video">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/wYnjsRY2SXs?si=KvZ1TZKi_ihdR_jj"
              title="YouTube video player" frameborder="0"
              allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
              referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
          </div>
        </div>
      </div>

      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              <b>Problem:</b> Developing autonomous home robots controlled by natural language has long been a pursuit
              of human. While
              advancements in large language models (LLMs) and embodied intelligence make this goal closer, several
              challenges persist: 1) the lack of a unified benchmark for more complex robot tasks. 2) limited evaluation
              methods and metrics. 3) data incompatibility between LLMs and mobile manipulation trajectories.
            </p>
            <p><b>EMMOE:</b> <b>E</b>mbodied <b>M</b>obile <b>M</b>anipulation in
              <b>O</b>pen <b>E</b>nvironments (<strong>EMMOE</strong>) requires robots to explore environments and
              perform various open-vocabulary mobile manipulation tasks
              based solely on language instructions and sensor observations.
            </p>
            <p>
              <b>Method:</b> We collect <strong>EMMOE-100</strong>, the
              first daily task dataset featuring Chain-of
              Thought (CoT) outputs, diverse task designs, detailed re-plan processes, SFT and Direct
              Preference Optimization (DPO) sub-datasets for LLM training. Additionally, we propose <b>three
                new metrics</b> for more overall assessment. Furthermore, we design
              <strong>HomieBot</strong>, a sophisticated agent system consists of LLM with DPO, light
              navigation and manipulation models, multiple error detection and adaptation mechanisms.
            </p>
            <p>
              <b>Evaluation:</b> We demonstrate how to construct new datasets based on EMMOE-100. Besides, we show
              HomieBot's performance and evaluations of different models and
              policies. Finally, we provide in-depth result analysis, visualizations, and case study.
            </p>
          </div>
        </div>
      </div>


    </div>
  </section>

  <!-- Benchmark -->
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Main Title -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">EMMOE Benchmark</h2>
        </div>
      </div>

      <!-- Subsections -->
      <div class="content has-text-justified">
        <h3 class="title is-4">EMMOE-100</h3>
        <p>
          <b>Data Collection.</b> We design 100 daily mobile manipulation tasks based on different episodes from
          Replica Challenge. To
          enhance task diversity and better align with human demands, there are five different task attributes, and one
          task can possess multiple attributes simultaneously. Then, we manually control a
          Fetch robot in Habitat-Lab 2.0 to complete all tasks and decompose trajectories into discrete
          subtasks. Each subtask consists of a pre-defined action, a target,
          and a low-level model selection.
        </p>
        <p>
          <b>Dataset Features.</b> All subtasks are annotated with four
          first-person view images and detailed reasoning processes. Moreover, we intentionally design some failed
          subtasks and provide corresponding re-plans to enhance dataset robustness. A key feature of EMMOE-100 is the
          emphasis on the reasoning process and
          interleaved execution. In the shown task, the agent must check the fridge first. Otherwise, even if the
          agent finally gets a banana from the kitchen, it will not be considered as a success.
        </p>

        <div style="display: flex; justify-content: space-between; align-items: flex-start; margin-top: 30px;">
          <figure
            style="flex: 1; text-align: center; margin: 0; display: flex; flex-direction: column; align-items: center;">
            <img src="images/dataset_column.png" class="interpolation-image" alt="Interpolate start reference image"
              style="height: 270px; width: auto;" />
            <figcaption style="font-size: 14px; color: #555;">
              <b>Task Attributes</b>
            </figcaption>
          </figure>

          <figure
            style="flex: 1; text-align: center; margin: 0; display: flex; flex-direction: column; align-items: center;">
            <img src="images/task.png" class="interpolation-image" alt="Interpolate start reference image"
              style="height: 270px; width: auto;" />
            <figcaption style="font-size: 14px; color: #555;">
              <b>Task example</b>
            </figcaption>
          </figure>
        </div>

        <h3 class="title is-4">Data Augmentation</h3>
        <p><b>SFT Augmentation.</b> Initially, all failed subtasks will be
          skipped as they are treated as junk data for the SFT dataset. Then we use GPT-4o
          to <b>rewrite</b> text descriptions of tasks and the analysis of each subtask for three times.</p>

        <p><b>DPO Augmentation.</b> For the i-th subtask and its input instruction I<sub>i</sub>,
          if the execution of output O<sub>i</sub> fails but the next output O<sub>i+1</sub>
          succeeds after re-plan, we will choose I<sub>i</sub> as the <i>prompt</i>,
          O<sub>i</sub> as the <i>rejected</i> and O<sub>i+1</sub> as the
          <i>chosen</i>. To obtain more data, we utilize following augmentation methods: 1) <b>Order
            Change:</b> shuffle
          the order of successful subtasks, treat successful output
          O<sub>i</sub> as <i>chosen</i> and O<sub>i+1</sub> as <i>rejected</i>. 2) <b>Action Change:</b> replace
          actions in subtasks with non-standard names or actions outside the available list.
          3) <b>Model Change:</b> replace the model choice with models of the same type in the model list.
        </p>
        <p>We also provide all data transformation and augmentation scripts <a
            href="https://huggingface.co/datasets/Dongping-Li/EMMOE-100/tree/main" target="_blank">here</a>.</p>

        <iframe src="https://huggingface.co/datasets/Dongping-Li/EMMOE-100/embed/viewer/default/SFT" frameborder="0"
          width="100%" height="560px"></iframe>


        <h3 class="title is-4">New Metrics</h3>
        <script type="text/javascript" async
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
          </script>

        <details style="margin-bottom: 20px;">
          <summary><b>Task Progress (TP)</b></summary>
          <p>
            To better measure the task execution process and the interrelations among subtasks, we
            propose Task Progress
            (TP), which is calculated as follows:
          </p>
          <p style="text-align: center;">
            \( TP = \max_{k_i \in K_T} \left( \frac{\text{len}(k_i^{\text{check}})}{\text{len}(k_i)} \right) \)
          </p>
          <p>
            A keypath is defined as an ordered node set of all necessary subtasks required to complete a task,
            k<sub>i</sub> is the i-th keypath in the keypath set K<sub>T</sub> for task T. Each task is assigned with
            several keypaths, representing different ways to complete the task. We strictly match the execution
            trajectory
            with the subtask nodes in k<sub>i</sub> in sequential order. Once the node in k<sub>i</sub> is successfully
            matched, it will be added to another ordered set k<sub>i</sub><sup>check</sup>, then the ratio between the
            length of k<sub>i</sub><sup>check</sup> and the length of k<sub>i</sub> will be recorded. This process will
            be
            repeated for all keypaths in K<sub>T</sub>, and the highest ratio will become the TP value of the
            trajectory.
            Only if TP reaches 100%, the trajectory will be considered successful. TP considers both the flexibility of
            the execution process and the relationships between every step. The way of using natural language and
            execution results to evaluate also simplifies new task design and enables evaluation in real-world
            scenarios,
            where writing PDDL files is impractical.
          </p>
        </details>

        <details style="margin-bottom: 20px;">
          <summary><b>Success End Rate (SER)</b></summary>
          <p>
            A fully autonomous robot should be able to actively terminate the execution at a
            proper moment. Otherwise,
            even if the task is already done, the robot may continue running and get stuck in an endless loop.
            Therefore,
            we propose Success End Rate (SER) to evaluate whether the agent has the ability to understand its current
            situation and reasonably determine the appropriate timing for task termination. The calculation method is as
            follows:
          </p>
          <p style="text-align: center;">
            \( SER = \frac{\text{len}(S) }{\sum_{t\in M} \text{count}_t(\text{end}) } \)
          </p>
          <p>
            t represents a single trajectory and M is the set of trajectories for all tasks. count<sub>t</sub>(end)
            equals
            1 if "End" is the final action of t or 0 otherwise. S is the set of successful trajectories, of which TP
            equals 100%. Then SER is calculated as the ratio of the number of successful trajectories to the number of
            trajectories that the agent deemed successful. Once SER reaches a certain threshold or even 100%, auxiliary
            methods or metrics are no longer needed to calculate SR.
          </p>
        </details>

        <details>
          <summary><b>Success Re-plan Rate (SRR)</b></summary>
          <p>
            Execution failures are common cases in the real world, especially in unfamiliar
            environments, which makes the
            ability to quickly adjust from failures and continuously adapt to new environments a crucial skill. To
            measure
            the adaptation and generalization abilities of the agent, we propose Success Re-plan Rate (SRR), which is
            calculated as follows:
          </p>
          <p style="text-align: center;">
            \( SRR = \frac{\sum_{t\in S} \text{count}_t(\text{replan}) }{\sum_{t\in M} \text{count}_t(\text{replan}) }
            \)
          </p>
          <p>
            count<sub>t</sub>(replan) is the number of re-plans in trajectory t. Other symbol definitions are the same
            as
            SER. SRR represents the effectiveness of re-planning and adaptability of the agent. When SRR reaches 100%,
            it
            indicates that the agent can adapt to all failures and then successfully complete the task.
          </p>
        </details>


      </div>
    </div>
  </section>


  <!-- HomieBot -->
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Main Title -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">HomieBot</h2>
        </div>
      </div>

      <!-- Subsections -->
      <div class="content has-text-justified">
        <h3 class="title is-4">System</h3>
        <p>HomieBot employs a hierarchical framework with communication mechanisms for interleaved execution. High-Level
          Planning (<b>HLP</b>) deals with embodied decision making and planning adaptation, while Low-Level Execution
          (<b>LLE</b>)
          handles continuous execution and provides feedback to HLP.
        </p>
        <figure style="text-align: center; margin: 0 auto; margin-bottom: 20px;">
          <img src="images/homiebot.jpg" class="interpolation-image" alt="Interpolate start reference image"
            style="width: 90%; height: auto;" />
          <figcaption style="font-size: 14px; color: #555;">
            <b>Overview of HomieBot.</b>
          </figcaption>
        </figure>
        <p>To facilitate communication with HLP and provide more detailed error information, we further classify common
          errors into four main types and several subtypes. 1) <b>Logical error.</b> <i>L1</i>: The agent's hands are
          already full but still attempts to pick/open/close; <i>L2</i>: The agent holds nothing but attempts to put;
          <i>L3</i>: The agent attempts to pick/put the object in a closed container; <i>L4</i>: The agent attempts to
          interact with a non-interactive object. 2) <b>Distance error.</b> <i>D1</i>: The agent stands too far and is
          unable to reach the target; <i>D2</i>: The agent is too close to the target and its arm is hindered from
          properly extending during interaction. 3) <b>Format Error.</b> <i>F1</i>: The output action or model is not in
          the
          available list; <i>F2</i>: The output target does not exist in the current scene or can not be recognized by
          low-level models. 4) <b>Execution Error.</b> <i>E1</i>: The limited capabilities of the low-level models or
          policies cause the failure; <i>E2</i>: Failed execution may result in the inventory information being
          accidentally updated.
        </p>


        <h3 class="title is-4">Training</h3>
        <p>
          We select 90 tasks from EMMOE-100, then we select
          Video-LLaVA-7B as our base model and conduct a two-stage training process. In the first
          stage, we fine-tune the base model. In the second stage,
          we align the fine-tuned model with DPO.
        </p>



      </div>
    </div>
  </section>


  <!-- Experiments -->
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Main Title -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Experiments</h2>
        </div>
      </div>

      <!-- Subsections -->
      <div class="content has-text-justified">
        <h3 class="title is-4">Settings</h3>
        <p><b>Metrics. </b>
          In addition to SR, TP, SER and SRR, we also choose
          Path Length Weighted SR (PLWSR) as one of our evaluation
          metrics. PLWSR measures the ability gap between the agent and the expert in successful
          trajectories.</p>

        <p><b>Baselines.</b> We select GPT-4o, Gemini-1.5-Pro,
          Qwen2-VL-7B and MiniCPM-V 2.6 as baseline high-level planners.
          By leveraging in-context learning abilities and minor adaptations, these models can be easily deployed in
          our system.
          For low-level executor, we extract individual skills from M3, modify their
          implementations, and pass environmental information between
          executions to ensure environmental consistency. Additionally, robotic arms will be reset
          after each execution to enhance the success rate.
        </p>

        <p><b>Evaluation Benchmarks.</b> All EMMOE-100 tasks will be used for evaluation, the remaining ten
          untrained tasks will serve as test set. Each task will be executed three times with a maximum step limit of 20
          each
          time, the <b>average</b> results will be used for the final calculation. (<b>!Note: we report the average
            success rate of a trajectory,
            rather than the
            number of tasks can be completed.</b>)</p>


        <h3 class="title is-4">Results</h3>
        <table cellspacing="0" cellpadding="10" style="width: 80%; margin: auto; text-align: center;">
          <caption style="font-size: 14px;">
            Performance comparison of different models on EMMOE-100 tasks.
          </caption>
          <thead>
            <tr style="background-color: #f2f2f2; line-height: 1.5;">
              <th>Model</th>
              <th>SR</th>
              <th>PLWSR</th>
              <th>TP</th>
              <th>SRR</th>
              <th>SER</th>
            </tr>
          </thead>

          <tbody>
            <tr style="line-height: 1.5;">
              <td>GPT-4o</td>
              <td>13.33</td>
              <td>10.51</td>
              <td>29.79</td>
              <td>3.57</td>
              <td>49.38</td>
            </tr>
            <tr style="line-height: 1.5;">
              <td>Gemini-1.5-Pro</td>
              <td>17.33</td>
              <td>14.79</td>
              <td>38.03</td>
              <td>3.39</td>
              <td>55.91</td>
            </tr>
            <tr style="line-height: 1.5;">
              <td>Qwen2-VL-7B</td>
              <td>1.00</td>
              <td>0.50</td>
              <td>16.55</td>
              <td>0.59</td>
              <td>25.00</td>
            </tr>
            <tr style="line-height: 1.5;">
              <td>MiniCPM-V 2.6</td>
              <td>0.67</td>
              <td>0.57</td>
              <td>14.45</td>
              <td>0.06</td>
              <td>40.00</td>
            </tr>
            <tr style="background-color: #f6fbfe; line-height: 1.5;">
              <td>HomieBot-7B (SFT)</td>
              <td>27.67</td>
              <td>20.88</td>
              <td>50.27</td>
              <td><b>9.23</b></td>
              <td>53.90</td>
            </tr>
            <tr style="background-color: #f6fbfe; line-height: 1.5;">
              <td>HomieBot-7B (SFT+DPO)</td>
              <td><b>30.30</b></td>
              <td><b>24.66</b></td>
              <td><b>51.39</b></td>
              <td>8.72</td>
              <td><b>60.81</b></td>
            </tr>
          </tbody>
        </table>
        <table cellspacing="0" cellpadding="10"
          style="width: 90%; margin: auto; margin-bottom: 20px; text-align: center; ">
          <caption style="margin-top: 10px; font-size: 14px;">
            Performance comparison of HomieBot on the training and test split.
          </caption>
          <thead>
            <tr style="background-color: #f2f2f2; line-height: 0.8;">
              <th rowspan="2" style="vertical-align: middle;">Model</th>
              <th colspan="5">Train split</th>
              <th colspan="5">Test split</th>
            </tr>
            <tr style="background-color: #f2f2f2; line-height: 0.8;">
              <th>SR</th>
              <th>PLWSR</th>
              <th>TP</th>
              <th>SRR</th>
              <th>SER</th>
              <th>SR</th>
              <th>PLWSR</th>
              <th>TP</th>
              <th>SRR</th>
              <th>SER</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>HomieBot (SFT)</td>
              <td>28.52</td>
              <td>21.49</td>
              <td>50.16</td>
              <td>9.59</td>
              <td>53.85</td>
              <td><b>20.00</b></td>
              <td><b>15.36</b></td>
              <td><b>51.19</b></td>
              <td><b>6.55</b></td>
              <td>54.55</td>
            </tr>
            <tr>
              <td>HomieBot (SFT+DPO)</td>
              <td><b>31.84</b></td>
              <td><b>25.82</b></td>
              <td><b>52.29</b></td>
              <td><b>9.69</b></td>
              <td><b>60.71</b></td>
              <td>16.67</td>
              <td>14.36</td>
              <td>43.39</td>
              <td>3.08</td>
              <td><b>62.50</b></td>
            </tr>
          </tbody>
        </table>
        <p><b>Empirical Findings</b></p>
        <ul>
          <li>Open-source models in
            similar size are struggling to complete EMMOE tasks without additional training.
          </li>
          <li>The performance improvement in SER is not so obvious. The strong commonsense and reasoning capabilities of
            GPT-4o and
            Gemini-1.5-Pro enable them to effectively decide when to end a trajectory.</li>
          <li>SFT version outperforms DPO version in SRR, reflecting that DPO may compromise the model's
            generalization and transferability.</li>
          <li>Observations in comparisons of different splits further confirm that DPO introduces generalization issues.
          </li>
          <li>SER is more related to the model's inherent ability, and remains stable for both versions across
            the training and test splits.</li>
        </ul>

        <!-- Pipeline Section -->
        <h3 class="title is-4">Visible Pipeline</h3>
        <div class="content has-text-justified">
          <p>After each trial, HomieBot will automatically save trajectory videos in different views. To make the
            entire pipeline more intuitive, we demonstrate raw experimental videos of three tasks. The first two tasks
            are executed by SFT version, and the third is
            executed by DPO version.</p>
          <div id="videoContainer" style="text-align: center;">
            <!-- 任务选择框 -->
            <select id="videoGroupSelector" onchange="loadSelectedGroup()"
              style="font-size: 16px; padding-left: 10px; height: 36px; width: 600px; border-radius: 5px; margin-bottom: 15px; margin-right: 20px;">
              <option value="0">Fetch two sugar boxes in the fridge and put them on the brown table, if there aren't
                enough sugar boxes in the fridge, find them elsewhere</option>
              <option value="1">Put all cracker_boxes on the tvstand to the sofa</option>
              <option value="2">Take the bowl on the table to the kitchen</option>
            </select>

            <!-- 视角选择框 -->
            <select id="viewSelector" onchange="loadSelectedGroup()"
              style="font-size: 16px; padding-left: 10px; height: 36px; width: 170px; border-radius: 5px; margin-bottom: 15px;">
              <option value="0">Third-Person View</option>
              <option value="1">First-Person View</option>
            </select>

            <div class="video-grid" id="videoGrid">
              <!-- 视频组将在这里动态生成 -->
            </div>
          </div>
        </div>

        <style>
          #videoGrid {
            display: flex;
            justify-content: center;
            gap: 20px;
            /* 调整视频之间的间距 */
          }

          .video-item {
            display: flex;
            flex-direction: column;
            align-items: center;
            /* 让视频和 caption 居中 */
          }

          .video-item video {
            width: 300px;
            /* 设置视频宽度 */
            height: 300px;
            /* 设置视频高度，确保是正方形 */
            border-radius: 8px;
          }

          .video-caption {
            margin-top: 5px;
            font-size: 16px;
            color: #555;
            text-align: center;
          }
        </style>

        <script>
          const allVideos = [
            [ // Task1
              ["videos/demo1/video_third_trial1.mp4", "videos/demo1/video_third_trial2.mp4", "videos/demo1/video_third_trial3.mp4"], // Third-Person View
              ["videos/demo1/video_head_trial1.mp4", "videos/demo1/video_head_trial2.mp4", "videos/demo1/video_head_trial3.mp4"]  // First-Person View
            ],
            [ // Task2
              ["videos/demo2/video_third_trial1.mp4", "videos/demo2/video_third_trial2.mp4", "videos/demo2/video_third_trial3.mp4"], // Third-Person View
              ["videos/demo2/video_head_trial1.mp4", "videos/demo2/video_head_trial2.mp4", "videos/demo2/video_head_trial3.mp4"]  // First-Person View
            ],
            [ // Task3
              ["videos/demo3/video_third_trial1.mp4", "videos/demo3/video_third_trial2.mp4", "videos/demo3/video_third_trial3.mp4"], // Third-Person View
              ["videos/demo3/video_head_trial1.mp4", "videos/demo3/video_head_trial2.mp4", "videos/demo3/video_head_trial3.mp4"]  // First-Person View
            ]
          ];

          function loadSelectedGroup() {
            const taskIndex = document.getElementById("videoGroupSelector").value;
            const viewIndex = document.getElementById("viewSelector").value;
            const selectedVideos = allVideos[taskIndex][viewIndex];

            const videoGrid = document.getElementById("videoGrid");
            videoGrid.innerHTML = ""; // 清空当前视频

            // 添加视频元素和 caption 到页面
            selectedVideos.forEach((videoSrc, index) => {
              let videoItem = document.createElement("div");
              videoItem.classList.add("video-item");

              let videoElement = document.createElement("video");
              videoElement.setAttribute("controls", "true");
              let sourceElement = document.createElement("source");
              sourceElement.setAttribute("src", videoSrc);
              sourceElement.setAttribute("type", "video/mp4");
              videoElement.appendChild(sourceElement);

              let caption = document.createElement("div");
              caption.classList.add("video-caption");
              caption.innerText = `Trial ${index + 1}`; // 自动生成 caption

              videoItem.appendChild(videoElement);
              videoItem.appendChild(caption);
              videoGrid.appendChild(videoItem);
            });
          }

          // 初始加载默认 Task & View 的视频
          // window.onload = loadSelectedGroup;
        </script>

      </div>
    </div>
  </section>



  <!-- Analysis -->
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Main Title -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">More Analysis and Evaluations</h2>
        </div>
      </div>

      <!-- Subsections -->
      <div class="content has-text-justified">
        <h3 class="title is-4">Error Analysis</h3>
        <p>To further explore the reasons for the overall low success rate and demonstrate how HomieBot can be used to
          simultaneously evaluate both HLP and LLE, we collect all errors occurred during experiments and conduct an
          in-depth analysis.</p>
        <figure>
          <img src="images/analysis.jpg" class="interpolation-image" alt="Interpolate start reference image"
            style="width: 100%; height: auto;" />
          <figcaption style="margin-top: 10px; font-size: 14px; color: #555;">
            <b>Error Statistics.</b> The left and right figures depict the proportion of each error type of each
            model in successful and failed trajectories respectively.
            Additionally, we indicate the proportion of total execution failures next to each model's name. Due to too
            few successful trajectories for Qwen2-VL and MiniCPM-V 2.6, their results will not be shown in the left
            figure.
          </figcaption>
        </figure>
        <p><b>Empirical Findings</b></p>
        <ul>
          <li>The primary obstructive factors are physical grounding
            failures and model hallucinations.
          </li>
          <li>
            LMM-trainable format data is significan, a small amount of data combined with our data augmentation
            methods can efficiently improve grounding issues.
          </li>
          <li>
            When the
            model's understanding ability is insufficient, it may fail to fully understand or even forget previous
            execution
            contents, ultimately resulting in meaningless outputs.
          </li>
          <li>
            Insufficiency of model's spatial perception ability can be amended through feedback information.
          </li>
        </ul>

      </div>

      <div class="content has-text-justified">
        <h3 class="title is-4">LLE Evaluation</h3>
        <p>Comprehensive error types allow us to evaluate HLP and LLE separately. We further classify <i>Execution
            Errors</i> based on action types and count total occurrences of each action. It is evident that <i>Pick</i>
          action has a significantly lower success rate and the
          highest proportion of execution errors compared to other actions.</p>
        <table style="width: 90%; border-collapse: collapse; text-align: center; margin: auto;">
          <caption style="margin-top: 10px; font-size: 14px;">
            Results of LLE evaluations. P represents the proportion of single action errors. SR here represents an
            average value as each skill is attempted up to three times per execution.
          </caption>
          <thead>
            <tr style="background-color: #f2f2f2;">
              <th>Metrics</th>
              <th>Go to</th>
              <th>Pick</th>
              <th>Place</th>
              <th>Open</th>
              <th>Close</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>P</td>
              <td>38.49</td>
              <td>49.77</td>
              <td>7.30</td>
              <td>3.32</td>
              <td>1.11</td>
            </tr>
            <tr>
              <td>SR</td>
              <td>45.32</td>
              <td>22.45</td>
              <td>40.97</td>
              <td>43.13</td>
              <td>36.45</td>
            </tr>
          </tbody>
        </table>

      </div>

      <div class="content has-text-justified">
        <h3 class="title is-4">Task Performance</h3>
        <p>We also evaluate SR for each type of task. <i>Short-Horizon</i> tasks are relatively easy due to
          straightforward processes and fewer
          overall steps. The most challenging are <i>Open-Ended</i> tasks, which usually have a very long total step
          count,
          with flexible processes and results, demanding powerful capabilities from both HLP and LLE models. </p>
        <table style="width: 100%; border-collapse: collapse; text-align: center;">
          <caption style="margin-top: 10px; font-size: 14px;">
            Average success rate for each type of task.
          </caption>
          <thead>
            <tr style="background-color: #f2f2f2;">
              <th>Model</th>
              <th>Short-Horizon</th>
              <th>Long-Horizon</th>
              <th>Open-Ended</th>
              <th>Logical</th>
              <th>Human-Style</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>HomieBot (SFT)</td>
              <td><b>43.75</b></td>
              <td>24.60</td>
              <td>18.52</td>
              <td>34.01</td>
              <td>25.24</td>
            </tr>
            <tr>
              <td>HomieBot (SFT+DPO)</td>
              <td>41.67</td>
              <td><b>28.11</b></td>
              <td>15.38</td>
              <td><b>35.86</b></td>
              <td><b>27.88</b></td>
            </tr>
          </tbody>
        </table>

      </div>

      <div class="content has-text-justified">
        <h3 class="title is-4">Case Study</h3>
        <p></p>
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <!-- Option Selection -->
            <select id="caseStudySelector" onchange="displayContent()"
              style="font-size: 16px; padding-left: 10px; height: 36px; width: 400px; border-radius: 5px;">
              <option value="0">Case 1: Successful Trajectory</option>
              <option value="1">Case 2: Terrible Grounding</option>
              <option value="2">Case 3: Limited LLE</option>
              <option value="3">Case 4: Meaningless Outputs</option>
              <option value="4">Case 5: Solvable D1 Error</option>
            </select>

            <!-- Text Content Display -->
            <div id="caseStudyContent" class="content has-text-justified">
              <textarea id="textArea" rows="10" cols="50" readonly class="textarea is-large"></textarea>
              <div id="explanation"></div>
            </div>
          </div>
        </div>
        <script>
          const caseStudyData = [
            {
              txtFile: "texts/case1.txt",
              explanation: "Here we show a successful trajectory of our HomieBot (DPO version). To facilitate understanding, we convert the dialogue data into the original EMMOE data format. As shown, even if errors occur, timely adjustments can be made through feedback, ensuring the correctness of the execution process."
            },
            {
              txtFile: "texts/case2.txt",
              explanation: "Here we show a terrible grounding problem issues during the inference of GPT4-o. Due to the lengthy path, we only highlight the most critical subtask outputs and their execution results to emphasize errors (following cases are handled similarly). We can see that the ungrounded output directly prevents the process to be continued. Even after informing the model that the object doesn't exist, the issue remains unresolved. The model continues to output incorrect objects or makes mistakes again after a few steps."
            },
            {
              txtFile: "texts/case3.txt",
              explanation: "Here we show a problem issues from the limited ability of low-level models. As we can see in step7 and step8, though high-level planner makes correct plans, execution still fails due to the limited ability of low-level models, this problem occurs repeatedly during whole process and finally leads to the failue the trajectory. "
            },
            {
              txtFile: "texts/case4.txt",
              explanation: "Here we show a case of Qwen2-VL-7B generating a large amount of meaningless outputs during the inference. As we can see, even though all subtasks are successful, the agent keep circling in place without making progress. These meaningless outputs quickly consume the remaining execution steps, ultimately causing the task to fail."
            },
            {
              txtFile: "texts/case5.txt",
              explanation: "Here we show how D1 error is solvable during the inference of Gemini-1.5-Pro. As we can see in step3, after a D1 error happens, a Go to action can effectively solve it and facilitate the success of the whole trajectory."
            }
          ];

          function displayContent() {
            // 确保选中的值是数字
            const selectedOption = parseInt(document.getElementById("caseStudySelector").value);
            const contentDisplay = document.getElementById("caseStudyContent");
            const explanationDisplay = document.getElementById("explanation");
            const textArea = document.getElementById("textArea");

            const selectedCase = caseStudyData[selectedOption];

            // 从txt文件读取内容并展示
            fetch(selectedCase.txtFile)
              .then(response => response.text())
              .then(text => {
                textArea.value = text; // 显示文本内容
              })
              .catch(error => {
                textArea.value = "无法加载内容，请稍后再试。";
              });

            // 显示解释文字
            explanationDisplay.innerHTML = "<p>" + selectedCase.explanation + "</p>";
          }

          // 默认加载第一个选项内容
          window.onload = function () {
            loadSelectedGroup();  // 调用 loadSelectedGroup() 函数
            displayContent();     // 调用 displayContent() 函数
          };

        </script>

        <style>
          /* Centering text area and explanation */
          #textArea {
            width: 100%;
            height: 300px;
            font-size: 16px;
            line-height: 1.6;
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            background-color: #f8f8f8;
            padding: 10px;
          }

          #explanation {
            text-align: justify;
            font-size: 16px;
            margin-top: 20px;
          }

          /* Centering the select element and label */
          .select {
            font-size: 16px;
            width: 100%;
            margin-top: 20px;
          }
        </style>

      </div>

    </div>
  </section>



  <section class="section">
    <div class="columns is-centered">
      <div class="container is-max-desktop content">
        <h2 class="title has-text-centered">BibTeX</h2>
        <pre>
  @misc{li2025emmoecomprehensivebenchmarkembodied,
    title={EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation in Open Environments}, 
    author={Dongping Li and Tielong Cai and Tianci Tang and Wenhao Chai and Katherine Rose Driggs-Campbell and Gaoang Wang},
    year={2025},
    eprint={2503.08604},
    archivePrefix={arXiv},
    primaryClass={cs.RO},
    url={https://arxiv.org/abs/2503.08604}
  }
        </pre>
      </div>
    </div>
  </section>

  
  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>